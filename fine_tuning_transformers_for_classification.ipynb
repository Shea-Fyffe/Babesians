{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine-tuning-transformers-for-classification",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN9oR5bYtUC+PeuCpAgVwMF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shea-Fyffe/Babesians/blob/master/fine_tuning_transformers_for_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_ZVjSGx2hzd"
      },
      "source": [
        "# Fine-tuning Transformer Models for Text Classification\n",
        "This colab is written in **Python** to illistrate the process of *fine-tuning* (see [Lui et al., 2020](https://doi.org/10.1007/978-981-15-5573-2)) state-of-the-art **Transformer** models to classify personality items. In this context the fine-tuning process involves training models with a relatively small amount of items with known trait labels. While this notebook demonstrates how these models can be used for text classification (i.e., content analysis; [Short et al., 2018](https://doi.org/10.1146/annurev-orgpsych-032117-104622)), transformer models have the potential to be implemented in other parts of the scale development process (e.g., *automated item generation*, *personality assessment*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPlCRN53ULva"
      },
      "source": [
        "### Libraries\n",
        "\n",
        "Colab comes with a large number of Python libraries pre-loaded. However, `Transformers` is not initially available in Colab. The `Transformers` library can be installed by using the code below.\n",
        "\n",
        "More information on the `Transformers` library can be seen [here](https://huggingface.co/transformers/quicktour.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7S6aRPS_w63"
      },
      "source": [
        "#@title Installing Transformers\n",
        "\n",
        "## Uncomment command below to install Transformers\n",
        "! pip install transformers\n",
        "! pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A25eSs8QUkS8"
      },
      "source": [
        "# load text classification modules from simpletransformers\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
        "\n",
        "# data libraries\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "# util libraries\n",
        "from scipy.special import softmax\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import gc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0eCZR2ngddV"
      },
      "source": [
        "### Using a GPU\n",
        "To speed things up you can use a *GPU* (*optional*).\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, confirm that you can connect to the GPU with tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXnDmXR7RDr2"
      },
      "source": [
        "# A helper function to check for a GPU\n",
        "# To check if you are able to use a GPU environment in Colab click the `Runtime` menu above, then select `Change Runtime Type`, the pick \"GPU\" for the `Hardware Accelerator` dropdown\n",
        "def get_gpu ():\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return torch.cuda.current_device()\n",
        "  else:\n",
        "    return -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE95PDPffa4-"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkXTg-LmUAkH"
      },
      "source": [
        "### Functions and Classes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model Classes\n",
        "#Multi-label classification\n",
        "class MultilabelTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = torch.nn.BCEWithLogitsLoss()\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), \n",
        "                        labels.float().view(-1, self.model.config.num_labels))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "#Self-evaluation evaluate on training set\n",
        "class IntrospectiveTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = torch.nn.BCEWithLogitsLoss()\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), \n",
        "                        labels.float().view(-1, self.model.config.num_labels))\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "IFYw-EprSt7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyQQt1yuCi2b"
      },
      "source": [
        "#@title Data Class\n",
        "class TextClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels:\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p21aZOBc9Pvh"
      },
      "source": [
        "#@title Load user-defined utility functions\n",
        "\n",
        "# Import Data function\n",
        "def import_data(path, text_col, label_col = None, enc = 'latin1'):\n",
        "  \"\"\"Import a CSV of sentences\n",
        "  \n",
        "  Args:\n",
        "    path: A csv file path\n",
        "    text_col: Name of column in csv containing sentences\n",
        "    label_col: Name of column containing labels\n",
        "    enc: File encoding to be used (optional)\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(path, encoding = enc)\n",
        "  \n",
        "  if label_col is None:\n",
        "    return df[text_col].tolist(), df\n",
        "  return df[text_col].tolist(), df[label_col].tolist(), df\n",
        "\n",
        "# Map labels to keys\n",
        "def map_labels_to_keys(labels, sort_labels = True):\n",
        "  \"\"\"Map text labels to integers\n",
        "  \n",
        "  Args:\n",
        "    labels: a list/vector of text labels\n",
        "    sort_labels: Sort labels alphabetically before recoding (optional)\n",
        "  \"\"\"\n",
        "  k = list(dict.fromkeys(labels))\n",
        "  if sort_labels:\n",
        "    k.sort()\n",
        "  labels_to_id = {k[i] : int(i) for i in range(0, len(k))}\n",
        "  labels_out = []\n",
        "  for j in labels:\n",
        "    labels_out.append(labels_to_id[j])\n",
        "  return labels_out, labels_to_id, len(k)\n",
        "\n",
        "# Update model directories\n",
        "def update_directories(model_output_dir):\n",
        "    file_time = datetime.datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")\n",
        "    model_output_dir = f'{model_output_dir}-{file_time}/'\n",
        "    out_file = f\"{model_output_dir}/{file_time}_results.csv\"\n",
        "    return out_file, model_output_dir\n",
        "\n",
        "# Get model for simple transformers\n",
        "def get_model(model_type):\n",
        "    if model_type == \"bert\":\n",
        "        model_name = \"bert-base-cased\"\n",
        "    elif model_type == \"roberta\":\n",
        "        model_name = \"roberta-large\"\n",
        "    elif model_type == \"distilbert\":\n",
        "        model_name = \"distilbert-base-cased-distilled-squad\"\n",
        "    elif model_type == \"distilroberta\":\n",
        "        model_type = \"roberta\"\n",
        "        model_name = \"cross-encoder/stsb-distilroberta-base\"\n",
        "    elif model_type == \"electra-base\":\n",
        "        model_type = \"electra\"\n",
        "        model_name = \"cross-encoder/ms-marco-electra-base\"\n",
        "    elif model_type == \"xlnet\":\n",
        "        model_name = \"xlnet-large-cased\"\n",
        "    elif model_type == \"bart\":\n",
        "        model_name = \"facebook/bart-large\"\n",
        "    elif model_type == \"deberta\":\n",
        "        model_type = \"debertav2\"\n",
        "        model_name = \"microsoft/deberta-v3-large\"\n",
        "    elif model_type == \"albert\":\n",
        "        model_name = \"albert-xlarge-v2\"\n",
        "    elif model_type == \"xlmroberta\":\n",
        "        model_name = \"xlm-roberta-large\"\n",
        "    else:\n",
        "        sys.exit(\"Study 2 model not found\")\n",
        "\n",
        "    return model_type, model_name\n",
        "\n",
        "# Format output data function\n",
        "def format_output_data(raw_outputs, test_case_ids = None, label_list = None, output_probs = True):\n",
        "  \"\"\"Format test data to be output to CSV\n",
        "  \n",
        "  Args:\n",
        "    raw_outputs: The raw_outputs from transformers model.predict()\n",
        "    test_case_ids: A list of test case ids (optional)\n",
        "    label_list: A list of *unique ordered* labels (optional)\n",
        "    output_probs: A boolean (True/False). If True (the default) will convert logit predictions to probabilities\n",
        "  \"\"\"\n",
        "  if output_probs:\n",
        "      out_df = softmax(raw_outputs, axis=1)\n",
        "  \n",
        "  out_df = pd.DataFrame(out_df)\n",
        "  \n",
        "  if label_list is not None:\n",
        "      out_df.columns = labels_list\n",
        "  \n",
        "  if test_case_ids is not None:\n",
        "      out_df.insert(0, 'id', test_case_ids)\n",
        "\n",
        "  return out_df\n",
        "  \n",
        "# compute metrics\n",
        "def compute_metrics(pred):\n",
        "  predictions, labels = pred\n",
        "  # calculate accuracy using sklearn's function\n",
        "  acc = accuracy_score(labels, predictions.argmax(-1))\n",
        "  return {'accuracy': acc,}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ekBE-p49KLC"
      },
      "source": [
        "#@title Fine-tuning function\n",
        "def fine_tune(model, text, labels, train_args, time_stamp_out_dir = True, max_seq_len = 'longest'):\n",
        "  \"\"\"Fine-tune a Transformers model for text classification\n",
        "  \n",
        "  Args:\n",
        "    model: a valid string representing the model_type\n",
        "    text: a list of sentences to use for fine-tuning\n",
        "    labels: a list of labels\n",
        "    train_args: dictionary of training arguments\n",
        "    time_stamp_out_dir: Update output directory to be time-stamped? (optional)\n",
        "    max_seq_len: string determining how to pad text sequences (optional)\n",
        "  \"\"\"\n",
        "  if time_stamp_out_dir:\n",
        "    _, new_out_dir = update_directories(train_args.output_dir)\n",
        "    train_args.output_dir = new_out_dir\n",
        "\n",
        "  _, model_name = get_model(model)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "  train_labels_indx, lab_to_id, num_labs = map_labels_to_keys(labels)\n",
        "  \n",
        "  if max_seq_len == 'longest':\n",
        "    train_encodings = tokenizer(text, truncation=True, padding=True)\n",
        "  else:\n",
        "    train_encodings = tokenizer(text, padding='max_len', max_length=max_seq_len)\n",
        "\n",
        "  train_dataset = TextClassificationDataset(train_encodings, train_labels_indx)\n",
        "    \n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\n",
        "      model_name, num_labels=num_labs, label2id = lab_to_id\n",
        "      )\n",
        "  \n",
        "  trainer = Trainer(model=model,\n",
        "      args = training_args,\n",
        "      train_dataset = train_dataset\n",
        "    )\n",
        " \n",
        "  trainer.train()\n",
        "    \n",
        "  return trainer, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdULdNEfUYb1"
      },
      "source": [
        "### Defining Variables\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We define our variables for purposes described in our research manuscripte. However, we encourage researchers and practitioners to try out alternative models. In addition, we wanted to minimize the tuning hyper-parameters during training as the aim of this research is to highlight Transformers in a baseline sense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8tlBiCW5mBy"
      },
      "source": [
        "#@title Define model to train\n",
        "transformer_model = \"bert\" #@param [\"deberta\", \"albert\", \"bert\", \"bart\", \"distilbert\",\"distilroberta\", \"electra\", \"roberta\", \"xlnet\", \"xlmroberta\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GAQl22KulEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcfb15ea-774a-42a6-b661-e812129cb79f"
      },
      "source": [
        "#@title Define training parameters\n",
        "\n",
        "# first we can initialized the ClassificationArguments object\n",
        "training_args = TrainingArguments(\n",
        "   num_train_epochs = 10,\n",
        "   learning_rate = 2e-5,\n",
        "   warmup_ratio = 0.10,\n",
        "   weight_decay = 0.01,\n",
        "   per_device_train_batch_size = 16,\n",
        "   seed = 42,\n",
        "   load_best_model_at_end=True,\n",
        "   evaluation_strategy=\"steps\", \n",
        "   output_dir = f\"{transformer_model}/outputs\",\n",
        ")\n",
        "\n",
        "# length to pad items to (~each word is 1.15 sequence units)\n",
        "SEQ_LEN = 32\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "using `logging_steps` to initialize `eval_steps` to 500\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU5mojBFURyy"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Fine-tuning A Transformer Model\n",
        "\n",
        "\n",
        "---\n",
        "This example demonstrates the fine-tuning process for the pupose of classifying personality items into their respective content domains.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoOpPwcYvTvc"
      },
      "source": [
        "### Importing and formatting Training Data\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "While there are several ways to import data into Colab ([see here](https://colab.research.google.com/notebooks/io.ipynb)), the most intuitive way is to upload a local `.csv` file. You can do this by:\n",
        "\n",
        "- Clicking the ***Files*** pane (the folder icon on the left)\n",
        "- Clicking the ***Upload to session storage*** icon (left-most icon)\n",
        "- Selecting the local data file you would like to use (e.g., `.csv`,`.tsv`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M28uNo1ssSF"
      },
      "source": [
        "For this example, I've imported a file named `fine-tuned-train-data.csv` (found on our GitHub repo in the directory `/raw-data/`)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD0E9EpLr3-V"
      },
      "source": [
        "#@title Importing custom training dataset\n",
        "\n",
        "# the import_data function will return a list of sentences and the original dataset\n",
        "train_text, train_labels, raw_data = import_data(\"fine-tune-train-data.csv\", \"text\", \"label\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob7mKIM_8Dpz"
      },
      "source": [
        "To properly import the training data we must specify the file path, column name containing our items, and column name containing our labels. Then, the `import_data()` returns three objects:\n",
        "\n",
        "- a list (vector) of items\n",
        "- a list (vector) of labels\n",
        "- a copy of our training data\n",
        "\n",
        "The code above assigns these to objects names `train_text`, `train_labels` and `raw_data` respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm6IMLaY-lu-"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "---\n",
        "\n",
        "Our fine-tune function only requires that we define the `Transformer model` we would like to use, as well as `input a vector of text` (i.e., personality items in this example), the `trait labels`, and the `training arguments` (which we defined in the **Variables** section of this tutorial). There are optional arguments, such as time-stamping the output directory, which would be a good ideal if training mulitple models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_tmG_6v-3oC"
      },
      "source": [
        "# tune the model using the labeled personality items\n",
        "fine_tuned_model, tokenizer = fine_tune(transformer_model, train_text, train_labels, training_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldtRKgWMtGIP"
      },
      "source": [
        "### Testing the model\n",
        "\n",
        "---\n",
        "\n",
        "Since we've fined tuned the model we can use the `.predict()` method to predict the labels of new text---for example---personality items, survey responses, and even performance evaluations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC4VyJrCsxQl"
      },
      "source": [
        "#### Import the test data\n",
        "First, we must import the test data (`fine-tune-test-data.csv`), making sure we only specify the `path` and `text_col` in the `import_data()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shc7rJxisSOZ"
      },
      "source": [
        "#@title Importing testing dataset\n",
        "\n",
        "# the import_data function will return a list of sentences and the original dataset if label is left blank\n",
        "test_text, test_data = import_data(\"fine-tune-test-data.csv\", \"text\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4Roz1kCifzq"
      },
      "source": [
        "# pre-process the test data before prediction\n",
        "test_encodings = tokenizer(test_text, truncation=True, padding=True)\n",
        "test_dataset = TextClassificationDataset(test_encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrMpanHluUfx"
      },
      "source": [
        "#### Predict the test items"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "9FTP0LlVsHol",
        "outputId": "bee87c9c-c633-41a7-918c-1272e71cea79"
      },
      "source": [
        "# predict the test set and return single label predictions and the raw logits\n",
        "predictions, _, _ = fine_tuned_model.predict(test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 119\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqiII1y0uahz"
      },
      "source": [
        "# we can format the output and save it\n",
        "out_test_df = format_output_data(predictions)\n",
        "out_test_df['predicted'] =  np.argmax(predictions, axis=1)\n",
        "out_test_df['model'] =  transformer_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdmSmmGAvhDr"
      },
      "source": [
        "# save results\n",
        "out_test_df.to_csv(f\"{transformer_model}-test-preds.csv\", index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWAyf4Rw1TGg"
      },
      "source": [
        "#### Saving the model\n",
        "fine-tuned models can also be saved and used for down-stream tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIdA0Ibs1fOB",
        "outputId": "30b151fd-f8fc-40db-ae60-7e0562b36d9b"
      },
      "source": [
        "fine_tuned_model.save_model(f\"{transformer_model}-fine-tuned-big5-personality\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to bert-fine-tuned-big5-personality\n",
            "Configuration saved in bert-fine-tuned-big5-personality/config.json\n",
            "Model weights saved in bert-fine-tuned-big5-personality/pytorch_model.bin\n"
          ]
        }
      ]
    }
  ]
}